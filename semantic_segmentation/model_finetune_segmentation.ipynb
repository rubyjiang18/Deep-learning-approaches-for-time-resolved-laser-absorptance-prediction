{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o86gn1nG6DQ3"
      },
      "source": [
        "\n",
        "\n",
        "### Image segementaion model (UNet) Fine-tune and Inference Code\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yj608dbS6E3"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WblwuyYcJzm"
      },
      "outputs": [],
      "source": [
        "!pip install torchvision --upgrade\n",
        "!pip install grad-cam\n",
        "!pip install timm\n",
        "!pip install imagecodecs\n",
        "!pip install pytorchtools\n",
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lME-yOdUPlEO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# read tiff\n",
        "import zipfile\n",
        "from tifffile import imread\n",
        "from torchvision.transforms import ToTensor\n",
        "import random\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6HJ95NBSHhB"
      },
      "outputs": [],
      "source": [
        "# dont forget to upload all .py files in \"utils\" and \"unet_model\" folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV2IGWBqOtsI"
      },
      "outputs": [],
      "source": [
        "from unet import UNet\n",
        "from keyholeDataset import Keyhole\n",
        "from loss import DiceBCEWithActivationLoss\n",
        "from augmentation import get_training_augmentation, preprocess\n",
        "from utils import plot_2_sidebyside, plot_3_sidebyside, save_model, save_loss_record\n",
        "from iou import iou_numpy\n",
        "from train import train\n",
        "from validation import validation\n",
        "import segmentation_models_pytorch as smp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyJE_hxsDUg2"
      },
      "source": [
        "## 2. Initiate a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU6DxoE9wfCp"
      },
      "outputs": [],
      "source": [
        "model = UNet(n_channels=3, n_classes=1, bilinear=1)\n",
        "model_name = \"UNet\"\n",
        "torch.cuda.empty_cache()\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MhlX03QBjkY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efmaWFtZG3oU"
      },
      "outputs": [],
      "source": [
        "# load pretrained weights\n",
        "# you need to upload the \"public_trained_models\" folders to your own google drive\n",
        "# this folder is available for download at https://drive.google.com/drive/folders/1PjvG199PSNGER255jMh35cCw4MV0Lp3G?usp=share_link\n",
        "# you can choose any one out the 5 available\n",
        "path = \"/content/drive/MyDrive/public_trained_models/Unet_segmentation/Unet_Split1_epoch_153\"\n",
        "checkpoint = torch.load(path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "for key, value in checkpoint.items():\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZupFMnMDY3g"
      },
      "source": [
        "## 3. load data + specify batch_size and epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RTL_ifQPfi4"
      },
      "outputs": [],
      "source": [
        "# create a folder called Keyhole, and import your annotated data to this folder\n",
        "# your data will be a zip folder uploaded to google drive\n",
        "# in case of fine-tuning, this will be your fine-tune data\n",
        "# one folder \"images\", one folder \"masks\", and one csv file containing how you split the data\n",
        "\n",
        "!mkdir Keyhole\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/keyhole_segmentation_data/keyhole_segmentation_data.zip', 'r') as zip:\n",
        "  zip.extractall(path='/content/Keyhole')\n",
        "\n",
        "# you need to create this csv file for your own fine-tune data\n",
        "# left colum is image name as in images folder, right colomn is 1train(80%), 0val(105),2test(10%)\n",
        "# change this to your csv file name\n",
        "csv_split_name = \"/image_and_split_1.csv\"\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "num_workers = 2 if cuda else 0\n",
        "print(\"Cuda = \" + str(cuda)+\" with num_workers = \"+str(num_workers))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyMdtFLAUNx1"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "\n",
        "train_dataset = Keyhole('/content/Keyhole/keyhole_segmentation_data',\n",
        "                        transform=get_training_augmentation(),\n",
        "                        preprocess=None,\n",
        "                        mode=\"train\",\n",
        "                        csv_name=csv_split_name)\n",
        "val_dataset = Keyhole('/content/Keyhole/keyhole_segmentation_data',\n",
        "                      transform=None,\n",
        "                      preprocess=None,\n",
        "                      mode=\"val\",\n",
        "                      csv_name=csv_split_name)\n",
        "test_dataset = Keyhole('/content/Keyhole/keyhole_segmentation_data',\n",
        "                       transform=None,\n",
        "                       preprocess=None,\n",
        "                       mode=\"test\",\n",
        "                       csv_name=csv_split_name)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Valid size: {len(val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-YnS4xUz8Uv"
      },
      "source": [
        "\n",
        "## 4. Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm9TIn-TzqWg"
      },
      "outputs": [],
      "source": [
        "# #del model\n",
        "# torch.cuda.empty_cache()\n",
        "# model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YkEnXPr6xB8Y"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (3, 576, 576))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXz1ANLOMIhs"
      },
      "outputs": [],
      "source": [
        " # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
        " # https://github.com/milesial/Pytorch-UNet/blob/master/train.py\n",
        "\n",
        " # you can experiment with lower lr bc finetune\n",
        "optimizer =  optim.RMSprop(model.parameters(), lr=1e-5, weight_decay=1e-8, momentum=0.99)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=15)  # goal: maximize Dice score\n",
        "grad_scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "criterion = DiceBCEWithActivationLoss() #nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DizdnLL_RMv2"
      },
      "outputs": [],
      "source": [
        "epochs = 50 # you can reduce epochs for fine-tuning\n",
        "amp = True\n",
        "train_loss_record= []\n",
        "val_loss_record= []\n",
        "lr_record = []\n",
        "# record the # of times lr changes\n",
        "prev_lr = 100; # 100 to simulate int.max_value\n",
        "lr_count = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LevPEXB4zqNQ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(0, epochs+1):\n",
        "  # lr - early stop\n",
        "  curr_lr = optimizer.param_groups[0]['lr'] # this value is 1e-5, you may need to adjust it depending on the test result\n",
        "  lr_record.append(curr_lr)\n",
        "  print('New peoch lr: ', curr_lr)\n",
        "  if curr_lr < prev_lr:\n",
        "    prev_lr = curr_lr\n",
        "    lr_count += 1\n",
        "  # if lr was reduced for the third time stop training\n",
        "  if (lr_count == 3):\n",
        "    print(\"Early Stop\")\n",
        "    save_model(model, epoch, model_name, optimizer, scheduler, grad_scaler, batch_size,\n",
        "               path=\"/content/drive/MyDrive/\") # you can change the path\n",
        "    save_loss_record(train_loss_record, val_loss_record, lr_record, model_name+\".csv\")\n",
        "    break\n",
        "  # train\n",
        "  train_loss = train(model, device, train_loader, optimizer, criterion, scheduler, grad_scaler, epoch, epochs, amp=True)\n",
        "  train_loss_record.append(train_loss)\n",
        "  # validation\n",
        "  val_loss = validation(model, device, val_loader, optimizer, criterion, scheduler, epoch, epochs, amp=True)\n",
        "  val_loss_record.append(val_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MBi_zwYYyyz"
      },
      "outputs": [],
      "source": [
        "save_model(model, epoch, model_name, optimizer, scheduler, grad_scaler, batch_size,\n",
        "               path=\"/content/drive/MyDrive/\") # you can change the path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJs1KSeSDs2H"
      },
      "source": [
        "## 5. Save model and and loss data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THDY-SGbCEGk"
      },
      "outputs": [],
      "source": [
        "save_model(model, epoch, model_name, optimizer, scheduler, grad_scaler, batch_size,\n",
        "           path = \"/content/\") # you can change this path to google drive path, it will be saved to your GD automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjnZnxe5Bqq5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(train_loss_record)\n",
        "plt.plot(val_loss_record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnF8fgaop6yH"
      },
      "outputs": [],
      "source": [
        "# you can save the above loss record as csv file\n",
        "save_loss_record(train_loss_record, val_loss_record, lr_record, model_name+\".csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgwu6hOQ44Dz"
      },
      "source": [
        "## 6. check test loss and IOU score\n",
        "\n",
        "#### you need to make a decison when to stop fine-tuning based on the train/val loss, you may also need to add more annotated data if the loss is not good enough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbEyfqG744_j"
      },
      "outputs": [],
      "source": [
        "test_loss = validation(model, device, test_loader, optimizer, criterion, scheduler, 0, epochs, amp=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YweotxL95G3o"
      },
      "outputs": [],
      "source": [
        "test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoaZREpw5IDe"
      },
      "outputs": [],
      "source": [
        "val_loss = validation(model, device, val_loader, optimizer, criterion, scheduler, 0, epochs, amp=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceDdBjzl5ejN"
      },
      "outputs": [],
      "source": [
        "val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5qnpD0L56BE"
      },
      "outputs": [],
      "source": [
        "train_loss = validation(model, device, train_loader, optimizer, criterion, scheduler, 0, epochs, amp=True)\n",
        "train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr7hv-fPfnT1"
      },
      "outputs": [],
      "source": [
        "pred_masks = []\n",
        "iou_record = []\n",
        "for i, batch in enumerate(test_loader):\n",
        "      print(\"i = \", i)\n",
        "      x = batch['image'].float().to(device)\n",
        "      y = batch['mask'].float().to(device)\n",
        "      assert(len(x) == len(y))\n",
        "      print(x.shape)\n",
        "      yp = model(x)\n",
        "\n",
        "      for i in range(len(x)):\n",
        "        x_ = x[i].unsqueeze(0)\n",
        "        y_ = y[i].unsqueeze(0)\n",
        "        yp_ = yp[i]\n",
        "        print(y_.shape)\n",
        "\n",
        "        # plot_2_sidebyside(\n",
        "        #               y_.detach().cpu().numpy()[0][0].astype(int),\n",
        "        #               (yp_.detach().cpu().numpy()[0]>0.5).astype(int))\n",
        "        iou_score = iou_numpy((yp_.detach().cpu().numpy()[0]>0.5).astype(int), y_.detach().cpu().numpy()[0][0].astype(int))\n",
        "        print(\"iou: \", iou_score)\n",
        "        iou_record.append(iou_score)\n",
        "\n",
        "      # print(\"yp shape\", yp.shape)#torch.Size([1, 1, 572, 572])\n",
        "      # plot_2_sidebyside(x.detach().cpu().numpy()[0][0],\n",
        "      #                 y.detach().cpu().numpy()[0][0])\n",
        "\n",
        "      # plot_2_sidebyside(\n",
        "      #                 y.detach().cpu().numpy()[0][0],\n",
        "      #                 (yp.detach().cpu().numpy()[0][0]>0.5).astype(int))\n",
        "\n",
        "\n",
        "      # plot_3_sidebyside(x.detach().cpu().numpy()[0][0],\n",
        "      #                 y.detach().cpu().numpy()[0][0],\n",
        "      #                 (yp.detach().cpu().numpy()[0][0]>0.5).astype(int))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE8eZv_vgBz6"
      },
      "outputs": [],
      "source": [
        "np.mean(iou_record)\n",
        "np.std(iou_record)\n",
        "# 0.8 or 80% IOU is good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqx5Sh5TVn-h"
      },
      "outputs": [],
      "source": [
        "save_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTn9uHyOcvW2"
      },
      "source": [
        "## 7. Inference on data with no masks\n",
        "\n",
        "Assuming you have loaded your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9IVfvx0cuiT"
      },
      "outputs": [],
      "source": [
        "# load the data, dont forget to change the path\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/keyhole_segmentation_data/keyhole_segmentation_data_no_mask.zip', 'r') as zip:\n",
        "  zip.extractall(path='/content/Keyhole')\n",
        "\n",
        "from keyholeDataset import KeyholeNoMask\n",
        "# we use KeyholeNoMask class to load this data\n",
        "# change the path\n",
        "infer_dataset = KeyholeNoMask('/content/Keyhole/keyhole_segmentation_infer_data')\n",
        "\n",
        "print(f\"infer_dataset size: {len(infer_dataset)}\")\n",
        "\n",
        "batch_size=2\n",
        "infer_loader = DataLoader(infer_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33I6bOVyet7g"
      },
      "outputs": [],
      "source": [
        "pred_masks = []\n",
        "for i, batch in enumerate(infer_loader):\n",
        "      print(\"i = \", i)\n",
        "      x = batch['image'].float().to(device)\n",
        "      print(x.shape)\n",
        "      yp = model(x)\n",
        "\n",
        "      for i in range(len(x)):\n",
        "        x_ = x[i].unsqueeze(0)\n",
        "        yp_ = yp[i]\n",
        "        pred_masks.append((yp_.detach().cpu().numpy()[0]>0.5).astype(int))\n",
        "        print(yp_.shape)\n",
        "\n",
        "        plot_2_sidebyside(\n",
        "                      x_.detach().cpu().numpy()[0][0].astype(int),\n",
        "                      (yp_.detach().cpu().numpy()[0]>0.5).astype(int))\n",
        "\n",
        "\n",
        "\n",
        "# Now save your pred_masks list, save each one as \"tif\" images\n",
        "# use the keyhole_feature_extraction code to get your features\n",
        "# https://github.com/rubyjiang18/Deep-learning-approaches-for-time-resolved-laser-absorptance-prediction/tree/main/keyhole_feature_extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh5FpldkeyGJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "o86gn1nG6DQ3"
      ],
      "gpuClass": "premium",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

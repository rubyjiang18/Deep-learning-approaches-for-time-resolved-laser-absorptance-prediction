{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o86gn1nG6DQ3"
      },
      "source": [
        "\n",
        "\n",
        "### Image segementaion model training code\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yj608dbS6E3"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WblwuyYcJzm"
      },
      "outputs": [],
      "source": [
        "!pip install torchvision --upgrade\n",
        "!pip install grad-cam\n",
        "!pip install timm\n",
        "!pip install imagecodecs\n",
        "!pip install pytorchtools\n",
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lME-yOdUPlEO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "import os \n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# read tiff\n",
        "import zipfile\n",
        "from tifffile import imread\n",
        "from torchvision.transforms import ToTensor\n",
        "import random\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV2IGWBqOtsI"
      },
      "outputs": [],
      "source": [
        "from unet import UNet\n",
        "from keyholeDataset import Keyhole\n",
        "from loss import DiceBCEWithActivationLoss \n",
        "from augmentation import get_training_augmentation, preprocess\n",
        "from utils import plot_2_sidebyside, plot_3_sidebyside, save_model, save_loss_record\n",
        "from iou import iou_numpy\n",
        "from train import train\n",
        "from validation import validation\n",
        "import segmentation_models_pytorch as smp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyJE_hxsDUg2"
      },
      "source": [
        "## 2. Initiate a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kFZtG3J8RwW"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU6DxoE9wfCp"
      },
      "outputs": [],
      "source": [
        "#model = torch.hub.load('milesial/Pytorch-UNet', 'unet_carvana', pretrained=True, scale=0.5)\n",
        "\n",
        "#model = UNet(n_channels=3, n_classes=1, bilinear=1)\n",
        "\n",
        "# resnet50, mobilenet_v2,\n",
        "model = smp.DeepLabV3( #.DeepLabV3 # .Unet\n",
        "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n",
        "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
        "    classes=1,                      # model output channels (number of classes in your dataset)\n",
        ")\n",
        "\n",
        "model_name = \"DLV3+Res50_Split5\"\n",
        "csv_split_name = \"/image_and_split_5.csv\"\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeLDW1162UH6"
      },
      "outputs": [],
      "source": [
        "# all models other than pure unet\n",
        "model.segmentation_head = nn.Sequential(*list(model.segmentation_head.children())[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efmaWFtZG3oU"
      },
      "outputs": [],
      "source": [
        "# #load model\n",
        "# path = \"/content/drive/MyDrive/DL_segmentation_models/UnetRes50_Split4_epoch_107\"\n",
        "# checkpoint = torch.load(path)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# for key, value in checkpoint.items():\n",
        "#     print(key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZupFMnMDY3g"
      },
      "source": [
        "## 3. load data + specify batch_size and epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RTL_ifQPfi4"
      },
      "outputs": [],
      "source": [
        "!mkdir Keyhole\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/DL_segmentation_data/keyhole_segmentation_data.zip', 'r') as zip:\n",
        "  zip.extractall(path='/content/Keyhole')\n",
        "\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "num_workers = 4 if cuda else 0\n",
        "print(\"Cuda = \" + str(cuda)+\" with num_workers = \"+str(num_workers))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyMdtFLAUNx1"
      },
      "outputs": [],
      "source": [
        "# need to write config file to make this part elegent\n",
        "batch_size = 2\n",
        "epochs = 300\n",
        "\n",
        "\n",
        "train_dataset = Keyhole('/content/Keyhole/keyhole_segmentation_data', \n",
        "                        transform=get_training_augmentation(),\n",
        "                        preprocess=None,\n",
        "                        mode=\"train\", \n",
        "                        csv_name=csv_split_name)\n",
        "val_dataset = Keyhole('/content/Keyhole/keyhole_segmentation_data', \n",
        "                      transform=None, \n",
        "                      preprocess=None, \n",
        "                      mode=\"val\", \n",
        "                      csv_name=csv_split_name)\n",
        "test_dataset = Keyhole('/content/Keyhole/keyhole_segmentation_data', \n",
        "                       transform=None, \n",
        "                       preprocess=None, \n",
        "                       mode=\"test\", \n",
        "                       csv_name=csv_split_name)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Valid size: {len(val_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQA5wFlLf0nG"
      },
      "outputs": [],
      "source": [
        "pred_masks = []\n",
        "iou_record = []\n",
        "for i, batch in enumerate(test_loader):\n",
        "      print(\"i = \", i)\n",
        "      x = batch['image'].float().to(device)\n",
        "      y = batch['mask'].float().to(device) \n",
        "      assert(len(x) == len(y))\n",
        "      print(x.shape)\n",
        "      yp = model(x)\n",
        "\n",
        "      for i in range(len(x)):\n",
        "        x_ = x[i].unsqueeze(0)\n",
        "        y_ = y[i].unsqueeze(0)\n",
        "        yp_ = yp[i]\n",
        "        print(y_.shape)\n",
        "\n",
        "        # plot_2_sidebyside( \n",
        "        #               y_.detach().cpu().numpy()[0][0].astype(int),\n",
        "        #               (yp_.detach().cpu().numpy()[0]>0.5).astype(int))\n",
        "        iou_score = iou_numpy((yp_.detach().cpu().numpy()[0]>0.5).astype(int), y_.detach().cpu().numpy()[0][0].astype(int))\n",
        "        print(\"iou: \", iou_score)\n",
        "        iou_record.append(iou_score)\n",
        "\n",
        "      # print(\"yp shape\", yp.shape)#torch.Size([1, 1, 572, 572])\n",
        "      # plot_2_sidebyside(x.detach().cpu().numpy()[0][0], \n",
        "      #                 y.detach().cpu().numpy()[0][0])\n",
        "      \n",
        "      # plot_2_sidebyside( \n",
        "      #                 y.detach().cpu().numpy()[0][0],\n",
        "      #                 (yp.detach().cpu().numpy()[0][0]>0.5).astype(int))\n",
        "                      \n",
        "      \n",
        "      # plot_3_sidebyside(x.detach().cpu().numpy()[0][0], \n",
        "      #                 y.detach().cpu().numpy()[0][0], \n",
        "      #                 (yp.detach().cpu().numpy()[0][0]>0.5).astype(int))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6Qqt1BmpdFS"
      },
      "outputs": [],
      "source": [
        "np.mean(iou_record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-77UvTWjuPfn"
      },
      "outputs": [],
      "source": [
        "np.std(iou_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-YnS4xUz8Uv"
      },
      "source": [
        "\n",
        "## 4. Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm9TIn-TzqWg"
      },
      "outputs": [],
      "source": [
        "# #del model\n",
        "# torch.cuda.empty_cache()\n",
        "# model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkEnXPr6xB8Y"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (3, 576, 576))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXz1ANLOMIhs"
      },
      "outputs": [],
      "source": [
        " # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
        " # https://github.com/milesial/Pytorch-UNet/blob/master/train.py\n",
        "optimizer =  optim.RMSprop(model.parameters(), lr=1e-5, weight_decay=1e-8, momentum=0.99) # 0.99\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=15)  # goal: maximize Dice score\n",
        "grad_scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "criterion = DiceBCEWithActivationLoss() #nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DizdnLL_RMv2"
      },
      "outputs": [],
      "source": [
        "# No early stop\n",
        "epochs = 300\n",
        "amp = True\n",
        "train_loss_record= []\n",
        "val_loss_record= []\n",
        "lr_record = []\n",
        "# record the # of times lr changes\n",
        "prev_lr = 100; # 100 to simulate int.max_value\n",
        "lr_count = -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpCvktdqm-9e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB-RG9MFlqDp"
      },
      "outputs": [],
      "source": [
        "#save_loss_record(train_loss_record, val_loss_record, lr_record, model_name+\".csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBgGcA-MjMxb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LevPEXB4zqNQ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(0, epochs+1):\n",
        "  # lr - early stop\n",
        "  curr_lr = optimizer.param_groups[0]['lr']\n",
        "  lr_record.append(curr_lr)\n",
        "  print('New peoch lr: ', curr_lr)\n",
        "  if curr_lr < prev_lr:\n",
        "    prev_lr = curr_lr\n",
        "    lr_count += 1\n",
        "  if (lr_count == 3):\n",
        "    print(\"Early Stop\")\n",
        "    save_model(model, epoch, model_name, optimizer, scheduler, grad_scaler, batch_size)\n",
        "    save_loss_record(train_loss_record, val_loss_record, lr_record, model_name+\".csv\")\n",
        "    break\n",
        "  # train\n",
        "  train_loss = train(model, device, train_loader, optimizer, criterion, scheduler, grad_scaler, epoch, epochs, amp=True)\n",
        "  train_loss_record.append(train_loss)\n",
        "  # validation\n",
        "  val_loss = validation(model, device, val_loader, optimizer, criterion, scheduler, epoch, epochs, amp=True)\n",
        "  val_loss_record.append(val_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J-ZIcRjBGDX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJs1KSeSDs2H"
      },
      "source": [
        "## 5. Save model and and loss data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THDY-SGbCEGk"
      },
      "outputs": [],
      "source": [
        "save_model(model, epoch, model_name, optimizer, scheduler, grad_scaler, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjnZnxe5Bqq5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(train_loss_record)\n",
        "plt.plot(val_loss_record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQMKze3EH6zI"
      },
      "outputs": [],
      "source": [
        "# save_model(model, 50, \"unet_test\", optimizer, scheduler, 1)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('loss.py') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pow5X7o2HkYO"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load(\"Unet_MobV3_Nopretrain_epoch_56\")\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnF8fgaop6yH"
      },
      "outputs": [],
      "source": [
        "save_loss_record(train_loss_record, val_loss_record, lr_record, model_name+\".csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS4JocbpUCMC"
      },
      "outputs": [],
      "source": [
        "# !cp DeepLabV3_Nopretrain_epoch_190 /content/drive/MyDrive/DL_segmentation_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y9ufxiJURx7"
      },
      "outputs": [],
      "source": [
        "save_model(model, 0, \"test\", optimizer, scheduler, grad_scaler, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgwu6hOQ44Dz"
      },
      "source": [
        "## 6. check test loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbEyfqG744_j"
      },
      "outputs": [],
      "source": [
        "test_loss = validation(model, device, test_loader, optimizer, criterion, scheduler, 0, epochs, amp=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YweotxL95G3o"
      },
      "outputs": [],
      "source": [
        "test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoaZREpw5IDe"
      },
      "outputs": [],
      "source": [
        "val_loss = validation(model, device, val_loader, optimizer, criterion, scheduler, 0, epochs, amp=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceDdBjzl5ejN"
      },
      "outputs": [],
      "source": [
        "val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5qnpD0L56BE"
      },
      "outputs": [],
      "source": [
        "train_loss = validation(model, device, train_loader, optimizer, criterion, scheduler, 0, epochs, amp=True)\n",
        "train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUTqRNj5GTYp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "o86gn1nG6DQ3"
      ],
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
